# 从线性回归到 MLP

本节通过简化激活函数为恒等函数，将神经元模型简化为线性模型，通过学习的方式获得线性模型的 参数，进而了解机器学习的基本方法。这样的模型一般被称为线性回归模型。

68

2. 从线性回归到 MLP 69 对于给定数据集 (x1,y1),(x2,y2),..., (xm ,ym )，其中 xi = (x1,x2,...,xn) 为输入， yi 为输出。线性回

归模型试图获得一个线性模型

f (x) = w1x1 + w2x2 + ... + wnxn + b 来尽可能的根据输入值预测实际输出的值，即 f (xi) ≃ yi。

利用欧式距离定义一个误差函数：

E = ( f (xi) − yi)2

通过数据学习的目的就是最小化这个误差。从数学上的角度来看，梯度的方向是函数增长速度最快的 方向，那么梯度的反方向就是函数减少最快的方向。

∂E

∂wi = 2( f (xi) − yi)xi i i

- 2∆ x

∂∂Eb = 2( f (xi) − yi) = 2∆ i

定义学习率为 η，更新参数 wi 的方式就是用 wi − η∆ ixi 替代 wi，这里将常数项 2 省去。这样根据数 据对 wi 不断更新以获得最优的线性模型参数的方法就是梯度下降法。

前边简单介绍了以线性回归模型为主的人工神经网络，下面进一步了解由神经元组成的网络。图 [8.1 ](#_page73_x72.00_y351.85)中的感知器模型可以看成是两个输入，一个输出的单层人工神经网络（输入层不涉及计算，不记入网络的

层数）。

` `![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.108.png)![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.109.png)

图 8.2: 线性回归模型

更常见的人工神经网络会有更多的层，中间的层称为隐藏层。如图 [8.3](#_page74_x72.00_y604.23)是含一个隐藏层的网络。由于线 性变换的组合仍然是线性变换，为了体现多层模型的价值，在每个感知器模型中都引入了非线性的激活函

数，在前文我们提及了 sigmoid 函数，目前更常用的是一个更简单的 ReLU（ Rectified Linear Unit）函数 （如图 [8.4](#_page75_x72.00_y64.23)）：

ReLU(x) = max(x, 0).

` `![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.110.png)![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.111.png)

<a name="_page74_x72.00_y604.23"></a>图 8.3: 多层感知器模型人工神经网络

70 第八章 人工神经网络

![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.112.jpeg)

<a name="_page75_x72.00_y64.23"></a>图 8.4: sigmoid 和 ReLU 函数
