### 2.1 电脑游戏中的智能

相信大部分读者都玩过电脑游戏，而电脑游戏中又有很大一部分是对抗性的。在电脑游戏中，玩家的对手-电脑-必然具有一定的“智能”才能使得游戏比较“有趣”。对于不同的游戏类型，所需要的智能算法也不同，这里仅讨论有限状态、公开信息的简单对抗游戏，例如猜拳游戏。通过这个大家都很熟悉的游戏，探讨可能涉及到的算法。

### 2.1.1 随机策略

就像人类有时会通过抛硬币来决定一些事情，计算机通过产生随机数的方法来进行决策是最简单而往往也很有效的方法。比如在猜拳游戏中，如果一方采用完全随机的出拳策略，理论上胜负也同样随机，因此会有 50％左右的胜率，不算好也不算差。

但目前大部分随机数发生器都是伪随机序列，虽然看起来分布比较均匀，却是通过公式产生，可以通过一定的算法来预测。经典的随机数产生方法是线性同余法（Linear Congruence Generator，LCG），其基本公式如下：

$X_{n+1}=(aX_{n}+c)modm$ (2.1)

线性同余法定义了三个常数：乘量 a、增量 c 和模数$m,X_{i}$为生成的随机数序列。为了保证产生的随机数可以获得最大周期（不重复长度），这三个常数应该符合：

1．m 与 c 互质

2．m 的所有质因数都能整除 a-1

3．若 m 是 4 的倍数，a-1 也是 4 的倍数

4．a、c 都是正数，还要比 m 小

对于比较简单的情况，游戏一方仅初始化一次随机数（$(X_{0}$的取值，即随机数的种子），游戏另一方就有可能猜到这个种子，并正确预测出后续产生的全部随机数，所谓的随机性也就完全丧失了。为了获得更好的随机数，可以采用更好的算法，或者采用可以产生真随机数的物理硬件来实现。

### 2.1.2 根据历史数据进行决策

在多次重复游戏中，游戏历史数据可以作为分析对象，简单的历史模式匹配就可以成为很好的策略：当各方面条件都相同时，采用一个过去曾经成功的动作获胜的概率会比较大。而如果考虑到对方也会分析历史，那么曾经采用的动作有可能被针对，相对要降低选择的概率。

部分多次重复的游戏运行过程可以抽象成马尔可夫链：从一个游戏状态到另外一个状态的转换状态概率可以视为常数。这样经过几轮游戏之后，就可以积累一定的数据，并根据统计信息计算状态转移的概率。

假设模型的状态空间为$S_{i},i=1,\cdots,n,$，动作空间为$A_{i},i=1,\cdots,m。$。则在特定状态下采取某种动作的概率可以表示为$P(A_{i}\vertS_{j})$。对于多阶情况则可以表示为：$P(A_{i})\vertS_{j_{1}},S_{j_{2}},\cdots,S_{jk})。$

对于猜拳游戏，根据双方的出拳状态，一共有 9 种可能，那下一次的出拳概率可以根据这 9 种情况采取不同的概率应对。而对方的出拳概率可以根据历史统计资料获得。上面只是考虑了一阶的情况，对于高阶情况则需要考虑更多的历史状态。

### 2.1.3 利用深度学习进行决策

前面的方法都是利用了人类的经验来设计策略，能不能通过人工学习的方式获得更好的策略呢？这也是目前人工智能研究中的重要分支，在很多领域也已经获得了很好的结果。例如著名的 Alpha-Zero 就可以完全通过学习，在围棋游戏中打败人类顶尖高手。
