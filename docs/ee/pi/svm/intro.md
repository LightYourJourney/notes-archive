# 支持向量机

对于给定的样本集，分类算法的目的就是要找到一个划分超平面，将不同类别的样本分开，但能实现这个目的的超平面可能有很多，要如何确定最好的那一个呢？图 5.1 是一个二维的例子，可以有不同的直线将两类样点分开。直观的看粗实线的分类效果最好，样点到直线的最短距离是最大，而且对于不同的样点相等。

<figure markdown="span">
  ![alt text](../../../assets/images/ee/pi/.png){ width="300" }
  <figcaption>图 5.1: 支持向量示例</figcaption>
</figure>

支持向量机（Support Vector Machine）就是利用这一数学特征寻找最优超平面的算法，距离超平面最 近的几个训练样本（图 5.1 中圈出的三个点）被称为支持向量。最后找到的超平面是可只根据支持向量来确定，因此 SVM 方法往往在样点数比较少的时候表现也很好，训练速度也比较快。

## 支持向量机的基本型

对于样本数据集 $(x_{i},y_{i}),y_{i} ∈ +1,−1$，划分超平面可以表示为：

$$
f (mathbfx) = wT x + b= 0 (5.1)
$$

样本空间中任意点到该超平面的距离可以写为：

$$
wT x + b
$$

如果这个超平面可以正确的对样本进行分类，可以令：

$$
wT i = +1; (5.3)
$$

其中使等号成立的样点就是支持向量，两个异类支持向量到超平面的距离和为：

$$

$$

寻求最佳分割超平面的过程就是最大化 γ 的优化问题，即：

$$
min 1∥w∥2, s.t. yi(wT xi + b) ≥ 1, i = 1 ,2,...,m (5.5)
$$

利用拉格朗日乘子法可以得到这个问题的等价问题，其拉格朗日函数可以写为：

$$
∑m
$$

令 $L(w,b,α)$ 对 $w$ 和 $b$ 的偏导为零可以解出 $α_{i}$ 的值，同时由于约束条件为不等式， $α_{i}$ 还必须满足 KKT（Karush-Kuhn-Tucker ）条件，即：

$$
αi ≥ 0;
$$

于是对于任意样本 $(x_{i},y_{i})$，若 $α_{i} = 0$ 则该样本对 $f(x_{i})$ 没有影响，若 $α_{i} > 0$ 则必有 $y_{i}f(x_{i}) = 1$，即样点在最大间隔边界上，是一个支持向量。

## 软间隔

如果训练样本并不是线性可分，前面介绍的算法就会失效，而缓解这一问题的一个方法就是采用“软间隔”，允许部分样本不满足约束条件  $y_{i}(w^{T}x_{i} + b) ≥ 1$，如图 5.2 所示，虽然在部分样本上会分类错误，但能获得一个可行的分类方案。

<figure markdown="span">
  ![alt text](../../../assets/images/ee/pi/.png){ width="300" }
  <figcaption>图 5.2: 软间隔示意图</figcaption>
</figure>

此时的优化目标为

$$
∑m
$$

其中 $l_{0/1}$ 定义如下，被称为“0/1 损失函数”：

$$

$$

其中 $C > 0$ 是一个常数，其取值越小，约束对优化目标的贡献越小，可以有更多的样点不满足约束条件，而 当 $C$ 取值为无穷大时，优化目标等效于“硬间隔”，所有样点都要满足约束条件。

然而 $l_{0/1}$ 函数非连续、非凸，不易进行数学分析。于是通常会用一些其他函数进行替代，常用的替代损失函数有：

- hinge 损失：$l_{hinge(z)} = max(0,1 − z)$
- 指数损失：$l_{exp(z)} = exp(−z)$
- 对数损失：$l_{log(z)} = log(1 + exp(− z))$

## 核函数

对于训练样本线性不可分的情况，还可以将样本从原始空间映射到一个更高维的特征空间，使得这些样本在新的特征空间内线性可分。数学上可以证明如果原始空间是有限维度，那么一定存在一个高维特性 空间使得样本线性可分。令 $ϕ(x)$ 为映射函数，则变换后的模型可表示为：

$$
f(x) = w^{T}ϕ(x) + b
$$

在通过拉格朗日方程求解的过程中，会涉及到高维空间的内积计算，但由于特征空间的维数可能很高，甚至是无穷维，直接计算内积通常很困难，因此定义如下的核函数：

$$
κ(xi, xj ) = ⟨ϕ(xi),ϕ(xj )⟩= ϕ(xi)T ϕ(xj )
$$

有了核函数就可以不必设计 $ϕ()$ 函数，因为仅靠核函数就可以完成计算并且根据下面的定理可以更容易的设计核函数。

令 $χ$ 为输入空间，$κ()$ 为定义在 $χ × χ$ 上的对称函数，则 $κ$ 是核函数当且仅当对于任意数据集 $D = x_{1},..., x_{m}$，“核矩阵”K 总是半正定的。

$$

$$

常用的核函数有如下几个：

- 线性核 $κ(x_{i},x_{j}) = x^T_i x_j$
- 多项式核 $κ(x_{i},x_{j}) = (x^T_i x)_d$，$d>=1$ 为多项式的系数
- 高斯核 $κ(x_{i},x_{j}) = exp(− ∥jxi2−σ,x2d∥2≥)，1σ为多项式的次数> 0 为高斯核的带宽
- 拉普拉斯核 $κ(x_{i},x_{j}) = exp(− ∥xi −σxj ∥2 )，σ > 0$
- Sigmoid 核 $κ(x_{i},x_{j}) = tanh(βx^T_i x_j + θ)$
