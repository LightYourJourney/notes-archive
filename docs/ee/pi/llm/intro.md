# 大语言模型

2023 年可以说是大模型爆发的一年，许多科技公司和研究机构都推出了各自的大语言模型，包括开源的 LLaMA 系列、OpenAI 的 GPT、清华大学的 GLM 等。

## GPT 模型

GPT（Generative Pre-trained Transformer）是由 OpenAI 开发的一种极其强大的自然语言处理模型，

2023 年 9 月已经推出了 4.0 版本，其中提供给网页端免费使用的是 3.5 版本。GPT 模型在训练过程中使用了大量的文本数据进行预训练。模型在训练的过程中，学会了语法结构、词义关系、常见知识等，使其具备了一定的常识和语言理解能力。GPT-3.5 是迄今为止最大的预训练语言模型之一，具有 1750 亿的参数。这意味着它有更大的容量来捕获复杂的语言模式和关系。

GPT 可以进行零样本学习（zero-shot learning）。这意味着即使没有针对特定任务进行微调，它也能够在某种程度上执行各种任务，如回答问题、翻译文本等。只需通过提供简单的指示或示例，模型就可以进行推理和生成。GPT 可以用于各种自然语言处理任务，包括生成文章、写作、对话系统、问答系统、代码生成等。

## ChatGPT

ChatGPT 是基于 GPT-3.5（或其他 GPT 变体）开发的一个特定应用，专注于进行自然语言对话和生成连续的对话文本。它被训练用于与用户进行交互，模拟人类对话的方式生成文本响应。ChatGPT 能够理解之前对话中的上下文，根据之前的对话内容进行响应。这使得它可以产生更准确和连贯的回复。类似于其他 GPT 模型，ChatGPT 的回复通常具有多样性，这意味着它可以以不同的方式回答相同的问题，从而增加了对话的趣味性和灵活性。

为了控制生成的输出，用户可以通过给出指导性提示或约束来引导 ChatGPT 的回复。例如，您可以明确指示模型以特定的角色回答、遵循某种语气或风格等。ChatGPT 可以在多种应用领域中使用，包括在线客服、虚拟助手、教育、娱乐、创意写作等。它可以与用户进行实时对话，为用户提供有关问题、信息、建议等方面的支持。

## ChatGLM

GLM（ Generative Language Model）是清华大学团队开发的大语言模型。GLM-130B 包含 1300 亿的参数，语言理解能力和 GPT-3 相当，在 MMLU 评测基准上，性能优于 GPT-3。ChatGLM 基于 GLM 模型进行微调，主要应用与对话场景，应用领域与 ChatGPT 基本相同。

ChatGLM 的 60 亿参数版本（ ChatGLM-6B）已经开源，由于参数量少，可以在消费级的 GPU 上运 行，方便本地使用和定制化。

ChatGLM 模型的接口可通过如下的 Python 代码进行调用：

```python linenums="1"
response, history = model.chat(tokenizer,
                               prompt,
                               history=history,
                               max_length=max_length if max_length else 2048,
                               top_p=top_p if top_p else 0.7,
                               temperature=temperature if temperature else 0.95)
```

其中 tokenizer 是大模型的分词器； prompt 是此次调用的提示词； history 是已经完成的对话历史； max_length 表示回复的最大长度； top_p 控制了生成文本时选择下一个词或标记的概率分布的截断点，

较低的 top_p 值会导致生成文本更加多样化，而较高的 top_p 值则会导致生成文本更加可靠； temperature 这个参数影响了模型生成文本时的随机性，较高的温度值会增加生成文本的随机性，使其更具多样性。
