2. 大语言模型 107

![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.171.jpeg)

<a name="_page112_x72.00_y64.23"></a>图 12.1: Transformer 网络结构

多头注意力（ Multi-Head Attention）：通过多个注意力头的并行计算，模型可以学习不同类型的关系， 从而更好地表示输入序列。

前馈神经网络（ Feedforward Neural Network）：每个编码器和解码器层都包含一个前馈神经网络，用 于在注意力计算后对表示进行进一步的非线性变换。

残差连接和层归一化：每个子层内都有残差连接和层归一化，有助于防止训练过程中的梯度消失问题。

Transformer 模型的优势在于能够并行计算，因此在训练过程中可以更高效地利用硬件资源。此外， Transformer 模型在预训练和微调的框架下，在许多自然语言处理任务中取得了显著的性能提升。例如， BERT、GPT 系列和 T5 等模型都是基于 Transformer 架构的重要变体，它们在各自的领域内都取得了领 先的地位。

2. 大语言模型

<a name="_page112_x72.00_y586.98"></a>2023 年可以说是大模型爆发的一年，许多科技公司和研究机构都推出了各自的大语言模型，包括开源 的 LLaMA 系列、OpenAI 的 GPT、清华大学的 GLM 等。

1. **GPT** 模型

GPT（ Generative Pre-trained Transformer）是由 OpenAI 开发的一种极其强大的自然语言处理模型，

目前 [1](#_page112_x86.35_y757.59)已经推出了 4.0 版本，其中提供给网页端免费使用的是 3.5 版本。 GPT 模型在训练过程中使用了大 量的文本数据进行预训练。模型在训练的过程中，学会了语法结构、词义关系、常见知识等，使其具备了一![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.172.png)

1<a name="_page112_x86.35_y757.59"></a>2023 年 9 月

108 第十二章 大语言模型简介

定的常识和语言理解能力。 GPT-3.5 是迄今为止最大的预训练语言模型之一，具有 1750 亿的参数。这意 味着它有更大的容量来捕获复杂的语言模式和关系。

GPT 可以进行零样本学习（ zero-shot learning）。这意味着即使没有针对特定任务进行微调，它也能 够在某种程度上执行各种任务，如回答问题、翻译文本等。只需通过提供简单的指示或示例，模型就可以进

行推理和生成。 GPT 可以用于各种自然语言处理任务，包括生成文章、写作、对话系统、问答系统、代码生

成等。

2. **ChatGPT**

ChatGPT 是基于 GPT-3.5（或其他 GPT 变体）开发的一个特定应用，专注于进行自然语言对话和生 成连续的对话文本。它被训练用于与用户进行交互，模拟人类对话的方式生成文本响应。 ChatGPT 能够 理解之前对话中的上下文，根据之前的对话内容进行响应。这使得它可以产生更准确和连贯的回复。类似

于其他 GPT 模型， ChatGPT 的回复通常具有多样性，这意味着它可以以不同的方式回答相同的问题，从 而增加了对话的趣味性和灵活性。

为了控制生成的输出，用户可以通过给出指导性提示或约束来引导 ChatGPT 的回复。例如，您可以 明确指示模型以特定的角色回答、遵循某种语气或风格等。 ChatGPT 可以在多种应用领域中使用，包括 在线客服、虚拟助手、教育、娱乐、创意写作等。它可以与用户进行实时对话，为用户提供有关问题、信息、建

议等方面的支持。

3. **ChatGLM**

GLM（ Generative Language Model）是清华大学团队开发的大语言模型。 GLM-130B 包含 1300 亿的 参数，语言理解能力和 GPT-3 相当，在 MMLU 评测基准上，性能优于 GPT-3。ChatGLM 基于 GLM 模 型进行微调，主要应用与对话场景，应用领域与 ChatGPT 基本相同。

ChatGLM 的 60 亿参数版本（ ChatGLM-6B）已经开源，由于参数量少，可以在消费级的 GPU 上运 行，方便本地使用和定制化。

ChatGLM 模型的接口可通过如下的 Python 代码进行调用：

1 response, history = model.chat(tokenizer, ![](Aspose.Words.b353301d-f3c7-44fc-a0ef-0183eb531768.173.png)2 prompt,

3 history=history,

4 max\_length=max\_length if max\_length else 2048,

5 top\_p=top\_p if top\_p else 0.7,

6 temperature=temperature if temperature else 0.95)

其中 tokenizer 是大模型的分词器； prompt 是此次调用的提示词； history 是已经完成的对话历史； max\_length 表示回复的最大长度； top\_p 控制了生成文本时选择下一个词或标记的概率分布的截断点，

较低的 top\_p 值会导致生成文本更加多样化，而较高的 top\_p 值则会导致生成文本更加可靠； temper- ature 这个参数影响了模型生成文本时的随机性，较高的温度值会增加生成文本的随机性，使其更具多样性。
