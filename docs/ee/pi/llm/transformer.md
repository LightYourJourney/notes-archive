# Transformer 模型

Google 在 2017 的论文：[*Attention is All You Need*](https://arxiv.org/abs/1706.03762) 中，提出了一个新的神经网络模型：[**Transformer**](https://zh.wikipedia.org/zh-hans/Transformer%E6%A8%A1%E5%9E%8B)。这个模型完全采用注意力机制（Attention）处理序列数据，在性能上相对之前的 RNN 模型有了很大的提高。目前几乎所有的自然语言模型都采用 Transformer 结构作为基本的组成单元，在翻译、问答、知识处理 等多个方面都取得了长足的进步。进而在 2023 年由 OpenAI 推出 ChatGPT 开始，推动了一波大语言模型广泛应用的热潮。

## 注意力机制

注意力机制（Attention Mechanism）是一种用于深度学习模型中的重要技术，旨在使模型能够在处理序列数据时更加关注输入序列中的不同部分，从而提高模型的性能。注意力机制最初被广泛用于序列到序列的翻译任务，但后来被扩展到各种其他任务中，如语音识别、图像处理和自然语言处理等。

注意力机制的核心思想是在生成输出的过程中，根据输入序列的不同部分来分配不同的权重。这使得模型可以根据输入序列的重要性动态地调整关注的位置，从而更好地捕捉输入序列之间的关系。在序列到序列的任务中，例如翻译，模型需要根据输入句子的每个单词来生成输出句子的每个单词，这就需要一种方法来确定输入句子中每个单词对于生成当前输出单词的重要性。

在注意力机制中，通常会计算一个注意力权重向量，该向量表示输入序列中每个位置的权重。这个权重向量可以基于不同的方法计算，但最常见的方法是使用点积、加性（一层神经网络）或缩放点积等。注意力机制允许模型根据输入的不同部分分配不同的关注权重，以便更好地处理序列数据。这对于捕捉长距离依赖关系、处理变长输入序列以及改善模型性能都非常有帮助。

## Transformer 网络结构

传统的序列模型，如循环神经网络（RNN）和长短时记忆网络（LSTM），在处理长序列数据时存在一些问题，例如难以捕捉长距离依赖关系和计算效率较低。Transformer 模型的设计就是为了解决这些问题。Transformer 模型的核心思想包括自注意力机制（Self-Attention）和多头注意力（Multi-Head Attention）。它将输入序列分别映射为查询（Q）、键（K）、值（V）向量，然后通过计算注意力权重，将值向量与权重相乘并求和，从而获得上下文信息。通过多个注意力头的组合，模型可以在不同的表示子空间中学习不同的关系。

Transformer 模型结构如图 12.1 所示，其主要的组成部分包括：

编码器（Encoder）：编码器由多个相同结构的层组成，每一层都包含了多头自注意力机制和前馈神经网络。编码器将输入序列映射为一系列高维表示，其中包含了输入序列的上下文信息。

解码器（Decoder）：解码器也由多个相同结构的层组成，除了编码器的结构外，解码器还包含一个多头注意力层，用于对编码器输出进行自注意力计算，以便更好地聚焦在输入序列的不同部分。

自注意力机制（Self-Attention）：这是 Transformer 模型的关键机制，用于计算输入序列中每个位置的权重，从而捕捉序列内不同位置的依赖关系。

<figure markdown="span">
  ![alt text](../../../assets/images/ee/pi/.png){ width="300" }
  <figcaption>图 12.1: Transformer 网络结构</figcaption>
</figure>

多头注意力（ Multi-Head Attention）：通过多个注意力头的并行计算，模型可以学习不同类型的关系，从而更好地表示输入序列。

前馈神经网络（ Feedforward Neural Network）：每个编码器和解码器层都包含一个前馈神经网络，用 于在注意力计算后对表示进行进一步的非线性变换。

残差连接和层归一化：每个子层内都有残差连接和层归一化，有助于防止训练过程中的梯度消失问题。

Transformer 模型的优势在于能够并行计算，因此在训练过程中可以更高效地利用硬件资源。此外，Transformer 模型在预训练和微调的框架下，在许多自然语言处理任务中取得了显著的性能提升。例如，BERT、GPT 系列和 T5 等模型都是基于 Transformer 架构的重要变体，它们在各自的领域内都取得了领先的地位。
