2. Transformer 模型

Google 在 2017 的论文： Attention is All You Need 中，提出了一个新的神经网络模型： Transformer。 这个模型完全采用注意力机制（ Attention）处理序列数据，在性能上相对之前的 RNN 模型有了很大的提 高。目前几乎所有的自然语言模型都采用 Transformer 结构作为基本的组成单元，在翻译、问答、知识处理 等多个方面都取得了长足的进步。进而在 2013 年由 OpenAI 推出 ChatGPT 开始，推动了一波大语言模 型广泛应用的热潮。

1. 注意力机制

注意力机制（ Attention Mechanism）是一种用于深度学习模型中的重要技术，旨在使模型能够在处理 序列数据时更加关注输入序列中的不同部分，从而提高模型的性能。注意力机制最初被广泛用于序列到序 列的翻译任务，但后来被扩展到各种其他任务中，如语音识别、图像处理和自然语言处理等。

注意力机制的核心思想是在生成输出的过程中，根据输入序列的不同部分来分配不同的权重。这使得 模型可以根据输入序列的重要性动态地调整关注的位置，从而更好地捕捉输入序列之间的关系。在序列到 序列的任务中，例如翻译，模型需要根据输入句子的每个单词来生成输出句子的每个单词，这就需要一种 方法来确定输入句子中每个单词对于生成当前输出单词的重要性。

在注意力机制中，通常会计算一个注意力权重向量，该向量表示输入序列中每个位置的权重。这个权 重向量可以基于不同的方法计算，但最常见的方法是使用点积、加性（一层神经网络）或缩放点积等。意力 机制允许模型根据输入的不同部分分配不同的关注权重，以便更好地处理序列数据。这对于捕捉长距离依 赖关系、处理变长输入序列以及改善模型性能都非常有帮助。

2. **Transformer** 网络结构

传统的序列模型，如循环神经网络（ RNN）和长短时记忆网络（ LSTM），在处理长序列数据时存在一 些问题，例如难以捕捉长距离依赖关系和计算效率较低。 Transformer 模型的设计就是为了解决这些问题。 Transformer 模型的核心思想包括自注意力机制（ Self-Attention）和多头注意力（ Multi-Head Attention）。 它将输入序列分别映射为查询（ Q）、键（ K）、值（ V）向量，然后通过计算注意力权重，将值向量与权重相乘 并求和，从而获得上下文信息。通过多个注意力头的组合，模型可以在不同的表示子空间中学习不同的关

系。

Transformer 模型结构如图 [12.1](#_page112_x72.00_y64.23) 所示，其主要的组成部分包括：

编码器（ Encoder）：编码器由多个相同结构的层组成，每一层都包含了多头自注意力机制和前馈神经 网络。编码器将输入序列映射为一系列高维表示，其中包含了输入序列的上下文信息。

解码器（ Decoder）：解码器也由多个相同结构的层组成，除了编码器的结构外，解码器还包含一个多头 注意力层，用于对编码器输出进行自注意力计算，以便更好地聚焦在输入序列的不同部分。

自注意力机制（ Self-Attention）：这是 Transformer 模型的关键机制，用于计算输入序列中每个位置的 权重，从而捕捉序列内不同位置的依赖关系。

106